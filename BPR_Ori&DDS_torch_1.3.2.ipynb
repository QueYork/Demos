{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0900197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/proj/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce33bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c666ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPR, self).__init__()\n",
    "        self.W = None             # user matrix\n",
    "        self.H = None             # item matrix\n",
    "        self.Wsc = None           # scorer\n",
    "        self.Hsc = None \n",
    "        \n",
    "        self.uid = None\n",
    "        self.iid = None\n",
    "        \n",
    "        self.user_items = {}\n",
    "        self.dev_user_items = {}\n",
    "        \n",
    "        self.rating_exp = None   # softmax sum\n",
    "        self.rating_exp_mul_H = None\n",
    "    \n",
    "    def preprocess(self, df, train_size=0.8, test_size=0.1):\n",
    "        df = df.rename(columns = {df.columns[0]: 'ori_uid', df.columns[1]: 'ori_iid', df.columns[2]: 'rating'})\n",
    "        df = df.groupby('ori_uid').filter(lambda x: x['ori_uid'].count()>=10)\n",
    "        uid_map = pd.DataFrame({\"ori_uid\": np.asarray(list(set(df.iloc[:,0].values)))})\n",
    "        uid_map[\"serial_uid\"] = uid_map.index\n",
    "        iid_map = pd.DataFrame({\"ori_iid\": np.asarray(list(set(df.iloc[:,1].values)))})\n",
    "        iid_map[\"serial_iid\"] = iid_map.index\n",
    "        \n",
    "        self.uid = uid_map[\"serial_uid\"].values\n",
    "        self.iid = iid_map[\"serial_iid\"].values\n",
    "        \n",
    "        df = df.merge(uid_map, left_on = 'ori_uid', right_on = 'ori_uid', how=\"left\")\n",
    "        df = df.merge(iid_map, left_on = 'ori_iid', right_on = 'ori_iid', how=\"left\")\n",
    "        df = df[['serial_uid', 'serial_iid', 'rating']]\n",
    "        \n",
    "        train, test = self._split(df, train_size)\n",
    "        test, dev = self._split(test, test_size / (1 - train_size))\n",
    "        return train, test, dev\n",
    "    \n",
    "    def _split(self, df, ratio):\n",
    "        train = pd.DataFrame(columns = df.columns, dtype=int)\n",
    "        test = pd.DataFrame(columns = df.columns, dtype=int)\n",
    "        for i in self.uid:\n",
    "            train_1, test_1 = train_test_split(df[df.iloc[:, 0] == i], train_size = ratio, shuffle = True, random_state = 5)\n",
    "            train = pd.concat([train, train_1])\n",
    "            test = pd.concat([test, test_1])\n",
    "        return train, test\n",
    "    \n",
    "    def generate_train_batch(self, batch, sets):\n",
    "        train = []\n",
    "        for b in range(batch):\n",
    "            u = self.uid[np.random.randint(0, len(self.uid))]\n",
    "            i = sets[u][np.random.randint(0, len(sets[u]))]\n",
    "            j = self.iid[np.random.randint(0, len(self.iid))]\n",
    "            while j in sets[u]:\n",
    "                j = self.iid[np.random.randint(0, len(self.iid))]\n",
    "            train.append([u, i, j])\n",
    "        return np.asarray(train) \n",
    "\n",
    "    def forward(self, uids, iids, device):\n",
    "        self.rating_exp = torch.zeros(len(self.uid)).to(device)\n",
    "        self.rating_exp_mul_H = torch.zeros([len(self.uid), self.H.shape[1]]).to(device)\n",
    "        \n",
    "        # 处理 idx 得到 embedded Wu Hi\n",
    "        emb_idxs = [self.user_items[uid] for uid in uids]\n",
    "        item_emb = nn.utils.rnn.pad_sequence([self.Hsc[emb_idx] for emb_idx in emb_idxs], batch_first=True)\n",
    "        user_emb = self.Wsc[uids][:, None, :]\n",
    "        \n",
    "        # 计算批次内 user_item 得分\n",
    "        user_item_exp_sc = torch.sum(item_emb * user_emb, dim = -1)\n",
    "        mask = (user_item_exp_sc != 0).type(torch.float32)\n",
    "        # 取指数， mask 保证补 0 位还是 0\n",
    "        user_item_exp_sc = torch.exp(user_item_exp_sc) * mask\n",
    "        \n",
    "        # 计算指数和\n",
    "        self.rating_exp_mul_H[uids] = torch.sum(user_item_exp_sc.unsqueeze(2).repeat(1, 1, self.H.shape[1]) * item_emb, dim = 1)\n",
    "        self.rating_exp[uids] = torch.sum(user_item_exp_sc, dim = 1)\n",
    "        #返回 softmax probablilty of item i among user_items\n",
    "        return torch.exp(torch.sum(self.Wsc[uids] * self.Hsc[iids], dim = 1)) / self.rating_exp[uids]\n",
    "        \n",
    "    def fit_dds(self, df, dev, k, stepsize=1, max_iter=10, batch=10000, dev_batch=5000, score_stepsize=1):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        \n",
    "        # 初始化 W，H\n",
    "        self.W = nn.Parameter(torch.rand(len(self.uid), k).to(device) * 0.01)   \n",
    "        self.H = nn.Parameter(torch.rand(len(self.iid), k).to(device) * 0.01) \n",
    "        \n",
    "        # 初始化 scorer\n",
    "        self.Wsc = torch.rand(len(self.uid), k).to(device) * 0.01    \n",
    "        self.Hsc = torch.rand(len(self.iid), k).to(device) * 0.01  \n",
    "        \n",
    "        # 创建字典：用户u对应他访问过的所有items集合    \n",
    "        self.user_items = df.groupby(df.columns[0])[df.columns[1]].apply(lambda x: np.array(x)).to_dict()\n",
    "        self.dev_user_items = dev.groupby(dev.columns[0])[dev.columns[1]].apply(lambda x: np.array(x)).to_dict()\n",
    "        \n",
    "        # 主模型优化器        \n",
    "        optimizer = optim.SGD([self.W, self.H], lr=stepsize)\n",
    "        \n",
    "        for x in range(max_iter):            \n",
    "            #取训练批次：uij三元组\n",
    "            uij = self.generate_train_batch(batch, self.user_items)\n",
    "            u = uij[:, 0]\n",
    "            i = uij[:, 1]\n",
    "            j = uij[:, 2]\n",
    "            u_emb = self.W[u]\n",
    "            i_emb = self.H[i]\n",
    "            j_emb = self.H[j]\n",
    "            \n",
    "            # 评分器概率分布，forward 返回 softmax 概率分布\n",
    "            score_prob = self.forward(u, i, device)     \n",
    "            \n",
    "            # 主模型参数更新\n",
    "            optimizer.zero_grad() \n",
    "            score_loss = -torch.sum(score_prob * torch.log(torch.sigmoid(torch.sum(u_emb * (i_emb-j_emb),dim = 1))))\n",
    "            bpr_loss = -torch.mean(torch.log(torch.sigmoid(torch.sum(u_emb * (i_emb - j_emb),dim = 1))))\n",
    "            score_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 训练集上 W,H 的梯度\n",
    "            W_grad_sum = self.W.grad.clone()\n",
    "            H_grad_sum = self.H.grad.clone()\n",
    "            \n",
    "            # 对数概率分布下 Wsc, Hsc 梯度\n",
    "            log_prob_Wsc_grad = torch.zeros((len(self.uid), k)).to(device)\n",
    "            log_prob_Hsc_grad = torch.zeros((len(self.iid), k)).to(device)\n",
    "            log_prob_Wsc_grad[u] = self.Hsc[i] - self.rating_exp_mul_H[u] / self.rating_exp[u].unsqueeze(1).repeat(1, k)\n",
    "            log_prob_Hsc_grad[i] = self.Wsc[u] * ((1 - score_prob).unsqueeze(1).repeat(1, k))\n",
    "            \n",
    "            #取 dev uij三元组\n",
    "            uij = self.generate_train_batch(dev_batch, self.dev_user_items)\n",
    "            u = uij[:, 0]\n",
    "            i = uij[:, 1]\n",
    "            j = uij[:, 2]\n",
    "            u_emb = self.W[u]\n",
    "            i_emb = self.H[i]\n",
    "            j_emb = self.H[j]\n",
    "            \n",
    "            # 计算 dev 集上 W,H 的梯度\n",
    "            optimizer.zero_grad()\n",
    "            dev_loss = -torch.sum(torch.log(torch.sigmoid(torch.sum(u_emb * (i_emb - j_emb),dim = 1))))\n",
    "            dev_loss.backward()\n",
    "            W_grad_dev_sum = self.W.grad.clone()    \n",
    "            H_grad_dev_sum = self.H.grad.clone()\n",
    "\n",
    "            # 计算 reward: reward 为 W,H 在训练集和 dev 集上的梯度积    \n",
    "            r_W = torch.sum(W_grad_sum * W_grad_dev_sum, dim=1)\n",
    "            r_H = torch.sum(H_grad_sum * H_grad_dev_sum, dim=1)\n",
    "            r_W = r_W.unsqueeze(1).repeat(1, k)\n",
    "            r_H = r_H.unsqueeze(1).repeat(1, k)\n",
    "\n",
    "            # Wsc，Hsc 更新\n",
    "            self.Wsc += score_stepsize * r_W * log_prob_Wsc_grad\n",
    "            self.Hsc += score_stepsize * r_H * log_prob_Hsc_grad\n",
    "\n",
    "            if ( x + 1 ) % 10 == 0:\n",
    "                print(f\"Iteration: {x+1}, BPR loss: {bpr_loss.item()}\")\n",
    "    \n",
    "    def fit_ori(self, df, k, stepsize=0.05, max_iter=10, batch=10000):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        \n",
    "        self.W = nn.Parameter(torch.rand(len(self.uid), k).to(device) * 0.01)    # 初始化 W，H\n",
    "        self.H = nn.Parameter(torch.rand(len(self.iid), k).to(device) * 0.01)  \n",
    "        \n",
    "        # 创建字典：用户u对应他访问过的所有items集合\n",
    "        self.user_items = df.groupby(df.columns[0])[df.columns[1]].apply(lambda x: np.array(x)).to_dict()\n",
    "        \n",
    "        optimizer = optim.SGD([self.W, self.H], lr=stepsize)     # 主模型优化器\n",
    "        for x in range(max_iter):\n",
    "            #取训练批次：uij三元组\n",
    "            uij = self.generate_train_batch(batch, self.user_items)\n",
    "            \n",
    "            u = uij[:, 0]\n",
    "            i = uij[:, 1]\n",
    "            j = uij[:, 2]\n",
    "            u_emb = self.W[u]\n",
    "            i_emb = self.H[i]\n",
    "            j_emb = self.H[j]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = -torch.sum(torch.log(torch.sigmoid(torch.sum(u_emb * (i_emb - j_emb),dim = 1))))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if ( x + 1 ) % 10 == 0:\n",
    "                print(f\"Iteration: {x+1}, BPR loss: {loss.item() / batch}\")\n",
    "    \n",
    "    def _predict(self, uid, items, n):\n",
    "        scores = torch.mv(self.H[items], self.W[uid])\n",
    "        if n > scores.shape[0]: \n",
    "            n = scores.shape[0]\n",
    "        top_N_val, top_N_idx = torch.topk(scores, k=n)\n",
    "        return list(zip(items[top_N_idx.cpu()], top_N_val.cpu()))\n",
    "\n",
    "    def NDCG(self, uid, test, n):         # 用模型排序+真实分数计算 DCG, 重排后计算 iDCG\n",
    "        test_user = test[test.iloc[:, 0] == uid]\n",
    "        rating = self._predict(uid, test_user.iloc[:, 1].values, n)\n",
    "        irating =sorted(test_user.iloc[:, 2].values, reverse=True)\n",
    "        dcg = 0\n",
    "        idcg = 0\n",
    "        if n > len(irating): n = len(irating)  \n",
    "        for i in range(n):\n",
    "            r = test_user[test_user.iloc[:, 1]==rating[i][0]].iloc[0, 2]\n",
    "            dcg += 1.0 * (2**r - 1) / math.log(i + 2, 2)\n",
    "            idcg += 1.0 * (2**irating[i] - 1) / math.log(i + 2, 2)\n",
    "        return dcg / idcg\n",
    "\n",
    "    def performance(self, test, n):      # Output recall@n, precision@n, NDCG@n\n",
    "        hit = 0\n",
    "        n_recall = 0\n",
    "        n_precision = 0\n",
    "        ndcg = 0\n",
    "        for i in self.uid:\n",
    "            # Items that User i hasn't tried in training set\n",
    "            unknown_items = np.setdiff1d(self.iid, self.user_items[i])\n",
    "            # Items that User i actually tried in testing set\n",
    "            known_items = test[test.iloc[:, 0] == i].iloc[:, 1].values\n",
    "            \n",
    "            #目标：预测 unknown items 中的top_N，若击中test中的items，则为有效预测\n",
    "            ru = self._predict(i, unknown_items, n)\n",
    "            \n",
    "            hit += sum(1 for item, pui in ru if item in known_items)\n",
    "            n_recall += len(known_items)\n",
    "            n_precision += n\n",
    "            ndcg += self.NDCG(i, test, n)\n",
    "            \n",
    "        recall = hit / (1.0 * n_recall)\n",
    "        precision = hit / (1.0 * n_precision)\n",
    "        ndcg /= len(self.uid)\n",
    "        return recall, precision, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319ffcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./ml-100k/u.data\", sep=\"\\t\", names=['user id', 'item id', 'rating', 'timestamp'])\n",
    "df2 = pd.read_csv(\"./ml-1m/ratings.dat\", sep=\"::\", names=['user id', 'item id', 'rating', 'timestamp'], engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cdfb1",
   "metadata": {},
   "source": [
    "### 100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1781ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79619, 3)\n",
      "(9942, 3)\n",
      "(10439, 3)\n"
     ]
    }
   ],
   "source": [
    "model1 = BPR()\n",
    "model2 = BPR()\n",
    "train1, test1, dev1 = model1.preprocess(df1)\n",
    "train2, test2, dev2 = model2.preprocess(df1)\n",
    "print(train1.shape)\n",
    "print(test1.shape)\n",
    "print(dev1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a055d2a",
   "metadata": {},
   "source": [
    "### 100K Pure BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26dbdb06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, BPR loss: 0.62280771484375\n",
      "Iteration: 20, BPR loss: 0.354079931640625\n",
      "Iteration: 30, BPR loss: 0.318456787109375\n",
      "Iteration: 40, BPR loss: 0.3061577392578125\n",
      "Iteration: 50, BPR loss: 0.2965654052734375\n",
      "Iteration: 60, BPR loss: 0.2616852294921875\n",
      "Iteration: 70, BPR loss: 0.2472632080078125\n",
      "Iteration: 80, BPR loss: 0.219330615234375\n",
      "Iteration: 90, BPR loss: 0.192760595703125\n",
      "Iteration: 100, BPR loss: 0.1887091552734375\n",
      "Iteration: 110, BPR loss: 0.17070421142578124\n",
      "Iteration: 120, BPR loss: 0.16379403076171875\n",
      "Iteration: 130, BPR loss: 0.154899951171875\n",
      "Iteration: 140, BPR loss: 0.149299267578125\n",
      "Iteration: 150, BPR loss: 0.1481927734375\n",
      "Iteration: 160, BPR loss: 0.14233681640625\n",
      "Iteration: 170, BPR loss: 0.1392462158203125\n",
      "Iteration: 180, BPR loss: 0.1381890625\n",
      "Iteration: 190, BPR loss: 0.1325078369140625\n",
      "Iteration: 200, BPR loss: 0.1253777099609375\n",
      "Iteration: 210, BPR loss: 0.1216952880859375\n",
      "Iteration: 220, BPR loss: 0.1112137939453125\n",
      "Iteration: 230, BPR loss: 0.10821766357421875\n",
      "Iteration: 240, BPR loss: 0.11471749267578125\n",
      "Iteration: 250, BPR loss: 0.1059791748046875\n",
      "Iteration: 260, BPR loss: 0.10233157958984375\n",
      "Iteration: 270, BPR loss: 0.09345342407226563\n",
      "Iteration: 280, BPR loss: 0.09117518310546875\n",
      "Iteration: 290, BPR loss: 0.09078895263671875\n",
      "Iteration: 300, BPR loss: 0.088901953125\n",
      "Iteration: 310, BPR loss: 0.0826513671875\n",
      "Iteration: 320, BPR loss: 0.08759103393554687\n",
      "Iteration: 330, BPR loss: 0.08146790161132812\n",
      "Iteration: 340, BPR loss: 0.0767776611328125\n",
      "Iteration: 350, BPR loss: 0.07376369018554688\n",
      "Iteration: 360, BPR loss: 0.07161253662109375\n",
      "Iteration: 370, BPR loss: 0.07403366088867187\n",
      "Iteration: 380, BPR loss: 0.06828992919921875\n",
      "Iteration: 390, BPR loss: 0.06729951171875\n",
      "Iteration: 400, BPR loss: 0.07047146606445312\n",
      "Iteration: 410, BPR loss: 0.062663818359375\n",
      "Iteration: 420, BPR loss: 0.0616942138671875\n",
      "Iteration: 430, BPR loss: 0.06744336547851562\n",
      "Iteration: 440, BPR loss: 0.06095506591796875\n",
      "Iteration: 450, BPR loss: 0.05792914428710937\n",
      "Iteration: 460, BPR loss: 0.05803570556640625\n",
      "Iteration: 470, BPR loss: 0.055959014892578124\n",
      "Iteration: 480, BPR loss: 0.05911466674804688\n",
      "Iteration: 490, BPR loss: 0.05771728515625\n",
      "Iteration: 500, BPR loss: 0.05166407470703125\n",
      "CPU times: total: 58.5 s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model1.fit_ori(train1, k = 50, max_iter = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85461f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.1366914103923648\n",
      "Recall@10: 0.12965198149265741\n",
      "NDCG@10: 0.8255023160063666\n",
      "CPU times: total: 2.38 s\n",
      "Wall time: 3.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 10\n",
    "rec, pre, ndcg = model1.performance(test1, n)\n",
    "print(f'Precision@{n}: {pre}')\n",
    "print(f'Recall@{n}: {rec}')\n",
    "print(f'NDCG@{n}: {ndcg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f9423",
   "metadata": {},
   "source": [
    "### 100K BPR + Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd574457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, BPR loss: 0.6657573580741882\n",
      "Iteration: 20, BPR loss: 0.49578118324279785\n",
      "Iteration: 30, BPR loss: 0.42297127842903137\n",
      "Iteration: 40, BPR loss: 0.37862229347229004\n",
      "Iteration: 50, BPR loss: 0.3530023694038391\n",
      "Iteration: 60, BPR loss: 0.3378726541996002\n",
      "Iteration: 70, BPR loss: 0.3367163836956024\n",
      "Iteration: 80, BPR loss: 0.31973254680633545\n",
      "Iteration: 90, BPR loss: 0.31463074684143066\n",
      "Iteration: 100, BPR loss: 0.30432575941085815\n",
      "Iteration: 110, BPR loss: 0.2773972749710083\n",
      "Iteration: 120, BPR loss: 0.2752606272697449\n",
      "Iteration: 130, BPR loss: 0.25736239552497864\n",
      "Iteration: 140, BPR loss: 0.2483510673046112\n",
      "Iteration: 150, BPR loss: 0.230539932847023\n",
      "Iteration: 160, BPR loss: 0.21782223880290985\n",
      "Iteration: 170, BPR loss: 0.21002554893493652\n",
      "Iteration: 180, BPR loss: 0.21014492213726044\n",
      "Iteration: 190, BPR loss: 0.1985919177532196\n",
      "Iteration: 200, BPR loss: 0.19996725022792816\n",
      "Iteration: 210, BPR loss: 0.18652160465717316\n",
      "Iteration: 220, BPR loss: 0.1823459267616272\n",
      "Iteration: 230, BPR loss: 0.17273718118667603\n",
      "Iteration: 240, BPR loss: 0.17536555230617523\n",
      "Iteration: 250, BPR loss: 0.1606588065624237\n",
      "Iteration: 260, BPR loss: 0.17119622230529785\n",
      "Iteration: 270, BPR loss: 0.15574683248996735\n",
      "Iteration: 280, BPR loss: 0.15702621638774872\n",
      "Iteration: 290, BPR loss: 0.1575598418712616\n",
      "Iteration: 300, BPR loss: 0.150117427110672\n",
      "Iteration: 310, BPR loss: 0.14069564640522003\n",
      "Iteration: 320, BPR loss: 0.14116023480892181\n",
      "Iteration: 330, BPR loss: 0.14841225743293762\n",
      "Iteration: 340, BPR loss: 0.14357194304466248\n",
      "Iteration: 350, BPR loss: 0.13298332691192627\n",
      "Iteration: 360, BPR loss: 0.13689526915550232\n",
      "Iteration: 370, BPR loss: 0.1371772140264511\n",
      "Iteration: 380, BPR loss: 0.1328279674053192\n",
      "Iteration: 390, BPR loss: 0.12963645160198212\n",
      "Iteration: 400, BPR loss: 0.12843765318393707\n",
      "Iteration: 410, BPR loss: 0.12609706819057465\n",
      "Iteration: 420, BPR loss: 0.12890490889549255\n",
      "Iteration: 430, BPR loss: 0.11860501766204834\n",
      "Iteration: 440, BPR loss: 0.12200284004211426\n",
      "Iteration: 450, BPR loss: 0.1106860339641571\n",
      "Iteration: 460, BPR loss: 0.1132149025797844\n",
      "Iteration: 470, BPR loss: 0.1137070283293724\n",
      "Iteration: 480, BPR loss: 0.12008300423622131\n",
      "Iteration: 490, BPR loss: 0.11629574000835419\n",
      "Iteration: 500, BPR loss: 0.1183183342218399\n",
      "CPU times: total: 6min 24s\n",
      "Wall time: 9min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model2.fit_dds(train1, dev1, k = 50, max_iter = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b0edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.15874867444326618\n",
      "Recall@10: 0.15057332528666265\n",
      "NDCG@10: 0.8317444912367737\n",
      "CPU times: total: 1.92 s\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 10\n",
    "rec, pre, ndcg = model2.performance(test1, n)\n",
    "print(f'Precision@{n}: {pre}')\n",
    "print(f'Recall@{n}: {rec}')\n",
    "print(f'NDCG@{n}: {ndcg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c144c",
   "metadata": {},
   "source": [
    "### 1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23925bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(797758, 3)\n",
      "(99692, 3)\n",
      "(102759, 3)\n"
     ]
    }
   ],
   "source": [
    "model3 = BPR()\n",
    "model4 = BPR()\n",
    "train3, test3, dev3 = model3.preprocess(df2)\n",
    "train4, test4, dev4 = model4.preprocess(df2)\n",
    "print(train3.shape)\n",
    "print(test3.shape)\n",
    "print(dev3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88834487",
   "metadata": {},
   "source": [
    "### 1M Pure BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6bfb96d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, BPR loss: 0.68849326171875\n",
      "Iteration: 20, BPR loss: 0.5787048828125\n",
      "Iteration: 30, BPR loss: 0.4073212890625\n",
      "Iteration: 40, BPR loss: 0.351703271484375\n",
      "Iteration: 50, BPR loss: 0.343032470703125\n",
      "Iteration: 60, BPR loss: 0.3304013671875\n",
      "Iteration: 70, BPR loss: 0.331921044921875\n",
      "Iteration: 80, BPR loss: 0.3256033203125\n",
      "Iteration: 90, BPR loss: 0.330059521484375\n",
      "Iteration: 100, BPR loss: 0.3271123046875\n",
      "Iteration: 110, BPR loss: 0.315700634765625\n",
      "Iteration: 120, BPR loss: 0.321971240234375\n",
      "Iteration: 130, BPR loss: 0.3211672119140625\n",
      "Iteration: 140, BPR loss: 0.31386025390625\n",
      "Iteration: 150, BPR loss: 0.311345263671875\n",
      "Iteration: 160, BPR loss: 0.31525048828125\n",
      "Iteration: 170, BPR loss: 0.3073806884765625\n",
      "Iteration: 180, BPR loss: 0.2932947265625\n",
      "Iteration: 190, BPR loss: 0.281919189453125\n",
      "Iteration: 200, BPR loss: 0.2668001708984375\n",
      "Iteration: 210, BPR loss: 0.245215478515625\n",
      "Iteration: 220, BPR loss: 0.2367917724609375\n",
      "Iteration: 230, BPR loss: 0.2350786376953125\n",
      "Iteration: 240, BPR loss: 0.2252342041015625\n",
      "Iteration: 250, BPR loss: 0.2232976318359375\n",
      "Iteration: 260, BPR loss: 0.2050648193359375\n",
      "Iteration: 270, BPR loss: 0.2102014404296875\n",
      "Iteration: 280, BPR loss: 0.209697412109375\n",
      "Iteration: 290, BPR loss: 0.1953183349609375\n",
      "Iteration: 300, BPR loss: 0.1937963623046875\n",
      "Iteration: 310, BPR loss: 0.1943654296875\n",
      "Iteration: 320, BPR loss: 0.1876852783203125\n",
      "Iteration: 330, BPR loss: 0.19207889404296874\n",
      "Iteration: 340, BPR loss: 0.18668985595703125\n",
      "Iteration: 350, BPR loss: 0.1831848388671875\n",
      "Iteration: 360, BPR loss: 0.178003955078125\n",
      "Iteration: 370, BPR loss: 0.1765302490234375\n",
      "Iteration: 380, BPR loss: 0.1815610595703125\n",
      "Iteration: 390, BPR loss: 0.16924439697265625\n",
      "Iteration: 400, BPR loss: 0.16482421875\n",
      "Iteration: 410, BPR loss: 0.16230609130859375\n",
      "Iteration: 420, BPR loss: 0.15879019775390624\n",
      "Iteration: 430, BPR loss: 0.1663079833984375\n",
      "Iteration: 440, BPR loss: 0.15794659423828125\n",
      "Iteration: 450, BPR loss: 0.16680791015625\n",
      "Iteration: 460, BPR loss: 0.1585174560546875\n",
      "Iteration: 470, BPR loss: 0.152787548828125\n",
      "Iteration: 480, BPR loss: 0.15142254638671876\n",
      "Iteration: 490, BPR loss: 0.1483734375\n",
      "Iteration: 500, BPR loss: 0.152998876953125\n",
      "CPU times: total: 1min 11s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model3.fit_ori(train3, k = 20, max_iter = 500, stepsize=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90fedb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.13413907284768212\n",
      "Recall@10: 0.0812703125626931\n",
      "NDCG@10: 0.8072148664429862\n",
      "CPU times: total: 18.1 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 10\n",
    "rec, pre, ndcg = model3.performance(test3, n)\n",
    "print(f'Precision@{n}: {pre}')\n",
    "print(f'Recall@{n}: {rec}')\n",
    "print(f'NDCG@{n}: {ndcg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920468ea",
   "metadata": {},
   "source": [
    "### 1M BPR + Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6491df15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, BPR loss: 0.6872056722640991\n",
      "Iteration: 20, BPR loss: 0.5791022181510925\n",
      "Iteration: 30, BPR loss: 0.466282457113266\n",
      "Iteration: 40, BPR loss: 0.41698288917541504\n",
      "Iteration: 50, BPR loss: 0.3944404125213623\n",
      "Iteration: 60, BPR loss: 0.37079060077667236\n",
      "Iteration: 70, BPR loss: 0.3681659400463104\n",
      "Iteration: 80, BPR loss: 0.3572051525115967\n",
      "Iteration: 90, BPR loss: 0.3441734313964844\n",
      "Iteration: 100, BPR loss: 0.35157909989356995\n",
      "Iteration: 110, BPR loss: 0.3355865776538849\n",
      "Iteration: 120, BPR loss: 0.32879626750946045\n",
      "Iteration: 130, BPR loss: 0.33184167742729187\n",
      "Iteration: 140, BPR loss: 0.31641504168510437\n",
      "Iteration: 150, BPR loss: 0.307462602853775\n",
      "Iteration: 160, BPR loss: 0.2963956594467163\n",
      "Iteration: 170, BPR loss: 0.29591602087020874\n",
      "Iteration: 180, BPR loss: 0.29209789633750916\n",
      "Iteration: 190, BPR loss: 0.27236083149909973\n",
      "Iteration: 200, BPR loss: 0.2658662796020508\n",
      "Iteration: 210, BPR loss: 0.26851415634155273\n",
      "Iteration: 220, BPR loss: 0.25188401341438293\n",
      "Iteration: 230, BPR loss: 0.24691493809223175\n",
      "Iteration: 240, BPR loss: 0.2438446283340454\n",
      "Iteration: 250, BPR loss: 0.23651713132858276\n",
      "Iteration: 260, BPR loss: 0.23220700025558472\n",
      "Iteration: 270, BPR loss: 0.22408239543437958\n",
      "Iteration: 280, BPR loss: 0.2121959924697876\n",
      "Iteration: 290, BPR loss: 0.20163387060165405\n",
      "Iteration: 300, BPR loss: 0.20474490523338318\n",
      "Iteration: 310, BPR loss: 0.1990000605583191\n",
      "Iteration: 320, BPR loss: 0.1977546662092209\n",
      "Iteration: 330, BPR loss: 0.19893255829811096\n",
      "Iteration: 340, BPR loss: 0.19021110236644745\n",
      "Iteration: 350, BPR loss: 0.18917448818683624\n",
      "Iteration: 360, BPR loss: 0.190087229013443\n",
      "Iteration: 370, BPR loss: 0.195067897439003\n",
      "Iteration: 380, BPR loss: 0.18711991608142853\n",
      "Iteration: 390, BPR loss: 0.1836761236190796\n",
      "Iteration: 400, BPR loss: 0.17731650173664093\n",
      "Iteration: 410, BPR loss: 0.18024778366088867\n",
      "Iteration: 420, BPR loss: 0.17265893518924713\n",
      "Iteration: 430, BPR loss: 0.17393650114536285\n",
      "Iteration: 440, BPR loss: 0.16719873249530792\n",
      "Iteration: 450, BPR loss: 0.1751570999622345\n",
      "Iteration: 460, BPR loss: 0.17206552624702454\n",
      "Iteration: 470, BPR loss: 0.17831289768218994\n",
      "Iteration: 480, BPR loss: 0.15757480263710022\n",
      "Iteration: 490, BPR loss: 0.16473126411437988\n",
      "Iteration: 500, BPR loss: 0.1623002588748932\n",
      "CPU times: total: 13min 5s\n",
      "Wall time: 18min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model4.fit_dds(train3, dev3, k=20, max_iter=500, stepsize=4, score_stepsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab28529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, BPR loss: 0.6871827244758606\n",
      "Iteration: 20, BPR loss: 0.5806105732917786\n",
      "Iteration: 30, BPR loss: 0.4621551036834717\n",
      "Iteration: 40, BPR loss: 0.41256240010261536\n",
      "Iteration: 50, BPR loss: 0.3887789249420166\n",
      "Iteration: 60, BPR loss: 0.37127685546875\n",
      "Iteration: 70, BPR loss: 0.35713833570480347\n",
      "Iteration: 80, BPR loss: 0.3605228066444397\n",
      "Iteration: 90, BPR loss: 0.35070475935935974\n",
      "Iteration: 100, BPR loss: 0.3333551585674286\n",
      "Iteration: 110, BPR loss: 0.33875972032546997\n",
      "Iteration: 120, BPR loss: 0.32622358202934265\n",
      "Iteration: 130, BPR loss: 0.3262203633785248\n",
      "Iteration: 140, BPR loss: 0.31632691621780396\n",
      "Iteration: 150, BPR loss: 0.32030531764030457\n",
      "Iteration: 160, BPR loss: 0.3096601366996765\n",
      "Iteration: 170, BPR loss: 0.299712598323822\n",
      "Iteration: 180, BPR loss: 0.29073184728622437\n",
      "Iteration: 190, BPR loss: 0.27969589829444885\n",
      "Iteration: 200, BPR loss: 0.2653302550315857\n",
      "Iteration: 210, BPR loss: 0.2544574737548828\n",
      "Iteration: 220, BPR loss: 0.25236785411834717\n",
      "Iteration: 230, BPR loss: 0.24815905094146729\n",
      "Iteration: 240, BPR loss: 0.2346946746110916\n",
      "Iteration: 250, BPR loss: 0.23128776252269745\n",
      "Iteration: 260, BPR loss: 0.21977216005325317\n",
      "Iteration: 270, BPR loss: 0.2234116941690445\n",
      "Iteration: 280, BPR loss: 0.21602435410022736\n",
      "Iteration: 290, BPR loss: 0.21140365302562714\n",
      "Iteration: 300, BPR loss: 0.2064797282218933\n",
      "Iteration: 310, BPR loss: 0.2013290375471115\n",
      "Iteration: 320, BPR loss: 0.19923213124275208\n",
      "Iteration: 330, BPR loss: 0.19335287809371948\n",
      "Iteration: 340, BPR loss: 0.1936219036579132\n",
      "Iteration: 350, BPR loss: 0.18589898943901062\n",
      "Iteration: 360, BPR loss: 0.18948498368263245\n",
      "Iteration: 370, BPR loss: 0.19060443341732025\n",
      "Iteration: 380, BPR loss: 0.19038483500480652\n",
      "Iteration: 390, BPR loss: 0.177034392952919\n",
      "Iteration: 400, BPR loss: 0.1768566370010376\n",
      "Iteration: 410, BPR loss: 0.17421075701713562\n",
      "Iteration: 420, BPR loss: 0.17727436125278473\n",
      "Iteration: 430, BPR loss: 0.1716386377811432\n",
      "Iteration: 440, BPR loss: 0.17657403647899628\n",
      "Iteration: 450, BPR loss: 0.17324253916740417\n",
      "Iteration: 460, BPR loss: 0.17303845286369324\n",
      "Iteration: 470, BPR loss: 0.16775742173194885\n",
      "Iteration: 480, BPR loss: 0.16561397910118103\n",
      "Iteration: 490, BPR loss: 0.18202777206897736\n",
      "Iteration: 500, BPR loss: 0.16579650342464447\n",
      "CPU times: total: 10min 40s\n",
      "Wall time: 13min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model4.fit_dds(train3, dev3, k=20, max_iter=500, stepsize=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbbb4e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.12145695364238411\n",
      "Recall@10: 0.07358664687236689\n",
      "NDCG@10: 0.8168880722578074\n",
      "CPU times: total: 17.8 s\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 10\n",
    "rec, pre, ndcg = model4.performance(test3, n)\n",
    "print(f'Precision@{n}: {pre}')\n",
    "print(f'Recall@{n}: {rec}')\n",
    "print(f'NDCG@{n}: {ndcg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, uids, iids, device):\n",
    "        self.rating_exp = torch.zeros(len(self.uid)).to(device)\n",
    "        self.rating_exp_mul_H = torch.zeros([len(self.uid), self.H.shape[1]]).to(device)\n",
    "\n",
    "        # 处理 idx 得到 embedded Wu Hi\n",
    "        emb_idxs = [self.user_items[uid] for uid in uids]\n",
    "        item_emb = nn.utils.rnn.pad_sequence([self.Hsc[emb_idx] for emb_idx in emb_idxs], batch_first=True)\n",
    "        user_emb = self.Wsc[uids][:, None, :]\n",
    "\n",
    "        # 计算批次内 user_item 得分\n",
    "        user_item_exp_sc = torch.sum(item_emb * user_emb, dim = -1)\n",
    "        max_sc_per_row = torch.max(user_item_exp_sc, 1).values\n",
    "\n",
    "        mask = (user_item_exp_sc != 0).type(torch.float32)\n",
    "        user_item_exp_sc = user_item_exp_sc - max_sc_per_row[:, None]\n",
    "\n",
    "        # 取指数， mask 保证补 0 位还是 0\n",
    "        user_item_exp_sc = torch.exp(user_item_exp_sc) * mask\n",
    "\n",
    "        # 计算指数和\n",
    "        self.rating_exp_mul_H[uids] = torch.sum(user_item_exp_sc.unsqueeze(2).repeat(1, 1, self.H.shape[1]) * item_emb, dim = 1)\n",
    "        self.rating_exp[uids] = torch.sum(user_item_exp_sc, dim = 1)\n",
    "\n",
    "        #返回 softmax probablilty of item i among user_items\n",
    "        return torch.exp(torch.sum(self.Wsc[uids] * self.Hsc[iids], dim = 1) - max_sc_per_row) / self.rating_exp[uids]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
