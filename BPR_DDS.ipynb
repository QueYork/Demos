{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9436bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b123448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR(object):\n",
    "    def __init__(self):\n",
    "        self.W = None             # user matrix\n",
    "        self.W_np = None\n",
    "        self.H = None             # item matrix\n",
    "        self.H_np = None\n",
    "        \n",
    "        self.W_scnp = None\n",
    "        self.H_scnp = None \n",
    "        self.rating_exp = {}\n",
    "        self.rating_exp_mul_H = {}\n",
    "        \n",
    "        self.uid = None            # uid,iid without duplicates\n",
    "        self.iid = None\n",
    "        \n",
    "        self.train_user_items = {}       # 用户u对应他访问过的所有items集合\n",
    "        self.dev_user_items = {}\n",
    "        \n",
    "        self.uid_dict = None      # serialize uid and iid\n",
    "        self.iid_dict = None      #  {(original id in dataset): (serial_idx)}\n",
    "        self.uid_dict_rev = None  # reverse key and value\n",
    "        self.iid_dict_rev = None  #  {(serial_idx): (original id in dataset)}\n",
    "        \n",
    "    def _split(self, df, ratio):\n",
    "        train = pd.DataFrame(columns = df.columns, dtype=int)\n",
    "        test = pd.DataFrame(columns = df.columns, dtype=int)\n",
    "        for i in self.uid:\n",
    "            train_1, test_1 = train_test_split(df[df.iloc[:, 0] == i], train_size = ratio)\n",
    "            train = pd.concat([train, train_1])\n",
    "            test = pd.concat([test, test_1])\n",
    "        return train, test\n",
    "    \n",
    "    def split(self, df, train_size=0.8, test_size = 0.1):\n",
    "        self.uid = np.asarray(list(set(df.iloc[:,0].values)))\n",
    "        self.iid = np.asarray(list(set(df.iloc[:,1].values)))\n",
    "        self.uid.sort()\n",
    "        self.iid.sort()\n",
    "        self.uid_dict = dict(zip(self.uid, [i for i in range(len(self.uid))]))\n",
    "        self.iid_dict = dict(zip(self.iid, [i for i in range(len(self.iid))]))\n",
    "        self.uid_dict_rev = {v : k for k, v in self.uid_dict.items()}\n",
    "        self.iid_dict_rev = {v : k for k, v in self.iid_dict.items()}\n",
    "        train, test = self._split(df, train_size)\n",
    "        test, dev = self._split(test, test_size / (1 - train_size))\n",
    "        return train, test, dev\n",
    "    \n",
    "    def generate_train_batch(self, batch, sets):\n",
    "        train = []\n",
    "        for b in range(batch):\n",
    "            u = self.uid[random.randint(0, self.uid.size - 1)]\n",
    "            i = sets[u][random.randint(0, sets[u].size - 1)]\n",
    "            j = self.iid[random.randint(0, self.iid.size - 1)]\n",
    "            while j in sets[u]:\n",
    "                j = self.iid[random.randint(0, self.iid.size - 1)]\n",
    "            train.append([self.uid_dict[u], self.iid_dict[i], self.iid_dict[j]])\n",
    "        return np.asarray(train)   \n",
    "        \n",
    "    def scorer_prob(self, uids, iids, batch):              # Softmax probability\n",
    "        prob = []                                          # uids, iids serial\n",
    "        for b in range(batch):\n",
    "            uid = uids[b].item()\n",
    "            iid = iids[b].item()\n",
    "            if uid not in self.rating_exp.keys():\n",
    "                r = 0\n",
    "                h = 0\n",
    "                for i in self.train_user_items[self.uid_dict_rev[uid]]:\n",
    "                    temp = np.exp(np.dot(self.W_scnp[uid], self.H_scnp[self.iid_dict[i]]))\n",
    "                    r += temp\n",
    "                    h += temp * self.H_scnp[self.iid_dict[i]]\n",
    "                self.rating_exp[uid] = r\n",
    "                self.rating_exp_mul_H[uid] = h\n",
    "            prob.append(np.exp(np.dot(self.W_scnp[uid], self.H_scnp[iid])) / self.rating_exp[uid])\n",
    "        prob = np.asarray(prob)\n",
    "        return torch.from_numpy(prob).cuda()\n",
    "    \n",
    "    def fit(self, df, dev, k, max_iter=5, epoch=500, batch = 512):\n",
    "        for u in self.uid:                                # 创建字典：用户u对应他访问过的所有items集合\n",
    "            self.train_user_items[u] = df[df.iloc[:, 0]==u].iloc[:, 1].values\n",
    "            self.dev_user_items[u] = dev[dev.iloc[:, 0]==u].iloc[:, 1].values\n",
    "        # 初始化 W，H    \n",
    "        self.W = torch.nn.Parameter(torch.rand(len(self.uid), k))      \n",
    "        self.H = torch.nn.Parameter(torch.rand(len(self.iid), k))\n",
    "        #初始化 scorer\n",
    "        self.W_scnp = np.random.rand(len(self.uid), k)*0.01\n",
    "        self.H_scnp = np.random.rand(len(self.iid), k)*0.01\n",
    "        \n",
    "        optimizer = torch.optim.Adam([self.W, self.H])\n",
    "        for x in range(max_iter):             # Use stochastic batch gradient descent method to solve W & H\n",
    "            loss = 0\n",
    "            for e in range(epoch):\n",
    "                #更新 W H\n",
    "                uij = self.generate_train_batch(batch, self.train_user_items)\n",
    "                u = torch.tensor(uij[:,0],dtype=torch.int32)\n",
    "                i = torch.tensor(uij[:,1],dtype=torch.int32)\n",
    "                j = torch.tensor(uij[:,2],dtype=torch.int32)\n",
    "                u_emb = self.W[u].cuda()\n",
    "                i_emb = self.H[i].cuda()\n",
    "                j_emb = self.H[j].cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                score = self.scorer_prob(u, i, batch)\n",
    "                \n",
    "                normal_loss = -torch.mean(torch.log(torch.sigmoid(torch.sum(u_emb * (i_emb - j_emb), dim = 1))))\n",
    "                normal_loss.backward()\n",
    "                grad_W = self.W.grad.numpy()\n",
    "                grad_H = self.H.grad.numpy()\n",
    "                self.W.grad = None\n",
    "                self.H.grad = None\n",
    "                \n",
    "                u_emb2 = self.W[u].cuda()\n",
    "                i_emb2 = self.H[i].cuda()\n",
    "                j_emb2 = self.H[j].cuda()\n",
    "                bprloss = -torch.mean(score * torch.log(torch.sigmoid(torch.sum(u_emb2 * (i_emb2 - j_emb2), dim = 1))))\n",
    "                bprloss.backward()\n",
    "                optimizer.step()\n",
    "                loss += bprloss \n",
    "                self.W.grad = None\n",
    "                self.H.grad = None\n",
    "                self.W_np = self.W.detach().numpy()\n",
    "                self.H_np = self.H.detach().numpy()\n",
    "                #更新 scorer 参数\n",
    "                uij_dev = self.generate_train_batch(batch, self.dev_user_items)\n",
    "                u_dev = torch.tensor(uij[:,0],dtype=torch.int32)\n",
    "                i_dev = torch.tensor(uij[:,1],dtype=torch.int32)\n",
    "                j_dev = torch.tensor(uij[:,2],dtype=torch.int32)\n",
    "                u_dev_emb = self.W[u_dev].cuda()\n",
    "                i_dev_emb = self.H[i_dev].cuda()\n",
    "                j_dev_emb = self.H[j_dev].cuda()\n",
    "                \n",
    "                dev_loss = -torch.mean(torch.log(torch.sigmoid(torch.sum(u_dev_emb * (i_dev_emb - j_dev_emb), dim = 1))))\n",
    "                dev_loss.backward()\n",
    "                grad_W_on_dev = self.W.grad.numpy()\n",
    "                grad_H_on_dev = self.H.grad.numpy()\n",
    "                \n",
    "                r_W = np.sum(grad_W * grad_W_on_dev, axis = 1)\n",
    "                r_H = np.sum(grad_H * grad_H_on_dev, axis = 1)\n",
    "                r_W = np.expand_dims(r_W, axis=0).repeat(k, axis = 0).T\n",
    "                r_H = np.expand_dims(r_H, axis=0).repeat(k, axis = 0).T\n",
    "                \n",
    "                log_prob_W_grad = np.zeros((len(self.uid), k))\n",
    "                log_prob_H_grad = np.zeros((len(self.iid), k))\n",
    "                for b in range(batch):\n",
    "                    u = uij[b,0]\n",
    "                    i = uij[b,1]\n",
    "                    log_prob_W_grad[u] = self.H_scnp[i] - self.rating_exp_mul_H[u] / self.rating_exp[u]\n",
    "                    log_prob_H_grad[i] = self.W_scnp[u]*(1 - score[b].cpu().numpy())\n",
    "                self.W_scnp += r_W * log_prob_W_grad\n",
    "                self.H_scnp += r_H * log_prob_H_grad\n",
    " \n",
    "                self.rating_exp = {}\n",
    "                self.rating_exp_mul_H = {}\n",
    "            print(f\"Iteration: {x+1}, BPR loss: {loss/epoch}\")   \n",
    "        \n",
    "#     def predict(self, user, n):      # Top-N recommendation\n",
    "#         top_N = []\n",
    "#         for i in self.iid:\n",
    "#             if i not in self.train_user_items[user]:\n",
    "#                 top_N.append((i, np.dot(self.W_np[self.uid_dict[user]], self.H_np[self.iid_dict[i]]))) \n",
    "#         return sorted(top_N, key=lambda s: s[1], reverse=True)[:n]\n",
    "    \n",
    "    def _predict(self, uid, items, n):\n",
    "        top_N = []\n",
    "        for i in range(len(items)):\n",
    "            user = self.uid_dict[uid]\n",
    "            item = self.iid_dict[items[i]]\n",
    "            top_N.append((items[i], np.dot(self.W_np[user], self.H_np[item])))\n",
    "        return sorted(top_N, key=lambda s: s[1], reverse=True)[:n]\n",
    "    \n",
    "    def NDCG(self, uid, test, n):         # 用模型排序+真实分数计算 DCG, 重排后计算 iDCG\n",
    "        test_user = test[test.iloc[:, 0] == uid]\n",
    "        rating = self._predict(uid, test_user.iloc[:, 1].values, n)\n",
    "        irating =sorted(test_user.iloc[:, 2].values, reverse=True)\n",
    "        dcg = 0\n",
    "        idcg = 0\n",
    "        if n > len(irating): n = len(irating)  \n",
    "        for i in range(n):\n",
    "            r = test_user[test_user.iloc[:, 1]==rating[i][0]].iloc[0, 2]\n",
    "            dcg += 1.0*(2**r - 1)/math.log(i + 2, 2)\n",
    "            idcg += 1.0*(2**irating[i] - 1)/math.log(i + 2, 2)\n",
    "        return dcg/idcg\n",
    "    \n",
    "    def performance(self, test, n):      # Output recall@n, precision@n, NDCG@n\n",
    "        hit = 0\n",
    "        n_recall = 0\n",
    "        n_precision = 0\n",
    "        ndcg = 0\n",
    "        for i in self.uid:\n",
    "            # Items that User i hasn't tried in training set\n",
    "            unknown_items = np.setdiff1d(self.iid, np.union1d(self.train_user_items[i], self.dev_user_items[i]))\n",
    "            # Items that User i actually tried in testing set\n",
    "            known_items = test[test.iloc[:, 0]==i].iloc[:, 1].values\n",
    "            #目标：预测 unknown items 中的top_N，若击中test中的items，则为有效预测\n",
    "            ru = self._predict(i, unknown_items, n)\n",
    "            for item ,pui in ru:\n",
    "                if item in known_items:\n",
    "                    hit += 1\n",
    "            n_recall += len(known_items)\n",
    "            n_precision += n\n",
    "            ndcg += self.NDCG(i, test, n)  \n",
    "        recall = hit / (1.0 * n_recall)\n",
    "        precision = hit / (1.0 * n_precision)\n",
    "        ndcg /= len(self.uid)\n",
    "        return recall, precision, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44094fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./ml-100k/u.data\", sep=\"\\t\", names=['user id', 'item id', 'rating', 'timestamp'])\n",
    "df2 = pd.read_csv(\"./ml-1m/ratings.dat\", sep=\"::\", names=['user id', 'item id', 'rating', 'timestamp'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97de26f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79619, 4)\n",
      "(9942, 4)\n",
      "(10439, 4)\n"
     ]
    }
   ],
   "source": [
    "model1 = BPR()\n",
    "train1, test1, dev1 = model1.split(df1)\n",
    "print(train1.shape)\n",
    "print(test1.shape)\n",
    "print(dev1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a19bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, BPR loss: 0.011047966614772448\n",
      "Iteration: 2, BPR loss: 0.0065634269697210125\n",
      "Iteration: 3, BPR loss: 0.005740098148399909\n",
      "Iteration: 4, BPR loss: 0.00531856343244564\n",
      "Iteration: 5, BPR loss: 0.005077416409057825\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train1, dev1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c716248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.07507953340402969\n",
      "Recall@10: 0.0712130356065178\n",
      "NDCG@10: 0.8140134587711596\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "rec, pre, ndcg = model1.performance(test1, n)\n",
    "print(f'Precision@{n}: {pre}')\n",
    "print(f'Recall@{n}: {rec}')\n",
    "print(f'NDCG@{n}: {ndcg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bcb5637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(797758, 4)\n",
      "(99692, 4)\n"
     ]
    }
   ],
   "source": [
    "model2 = BPR()\n",
    "train2, test2, dev2 = model2.split(df2)\n",
    "print(train2.shape)\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae202dbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, BPR loss: 0.010513865191826304\n",
      "Iteration: 2, BPR loss: 0.006948783700702833\n",
      "Iteration: 3, BPR loss: 0.0061021069422561545\n",
      "Iteration: 4, BPR loss: 0.0057847897874899615\n",
      "Iteration: 5, BPR loss: 0.005546645937397752\n"
     ]
    }
   ],
   "source": [
    "model2.fit(train2, dev2, k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f1de0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.08932119205298013\n",
      "Recall@10: 0.0541166793724672\n",
      "NDCG@10: 0.8189502763082991\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "rec, pre, ndcg = model2.performance(test2, n)\n",
    "print(f'Precision@{n}: {pre}')\n",
    "print(f'Recall@{n}: {rec}')\n",
    "print(f'NDCG@{n}: {ndcg}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
